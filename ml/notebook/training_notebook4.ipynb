{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training\n",
    "\n",
    "Target of this code is to train a CNN network to classify images of a digital readout to the digits 0 to 9. \n",
    "\n",
    "### Preparing the training\n",
    "* First all libraries are loaded\n",
    "    * It is assumed, that they are installed during the Python setup\n",
    "* matplotlib is set to print the output inline in the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########### Basic Parameters for Running: ################################\n",
    "    \n",
    "TFliteNamingAndVersion = \"testing\"   # Used for tflite Filename\n",
    "Training_Percentage = 0.3              # 0.0 = Use all Images for Training\n",
    "Epochs = 200\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import History, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image \n",
    "from pathlib import Path\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "loss_ges = np.array([])\n",
    "val_loss_ges = np.array([])\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "* The data is expected in the \"Input_dir\"\n",
    "* Inside subdirectories are expected from 0, 1, ... 9 in which the pictures are sorted according to their values (=category)\n",
    "* Picture size must be 15x25 with 3 color channels (RGB)\n",
    "* The filename can be arbitrary\n",
    "\n",
    "* The images are stored in the x_data[]\n",
    "* The expected category for each image in the corresponding y_data[]\n",
    "\n",
    "* The last step is a shuffle (from sklearn.utils) and split the data into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converter.py  \u001b[0m\u001b[01;34mlabelled_bmp_data_equal_combined_final\u001b[0m/\r\n",
      "\u001b[01;34mdata1\u001b[0m/        \u001b[01;31mlabelled_bmp_data_equal_combined_final.zip\u001b[0m\r\n",
      "\u001b[01;34mdata2\u001b[0m/        \u001b[01;34mlabelled_bmp_data_equal_combined_orig\u001b[0m/\r\n",
      "\u001b[01;34mdata3\u001b[0m/        \u001b[01;31mlabelled_bmp_data.zip\u001b[0m\r\n",
      "\u001b[01;34mdata4\u001b[0m/        \u001b[01;31mlabelled_grayscale_png.zip\u001b[0m\r\n",
      "\u001b[01;34mdata5\u001b[0m/        \u001b[01;34msample\u001b[0m/\r\n",
      "\u001b[01;34mfine\u001b[0m/         \u001b[01;34msample_jpg\u001b[0m/\r\n",
      "\u001b[01;31mfine.zip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls ../data/raw_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9278, 25, 15, 3)\n",
      "(9278, 10)\n"
     ]
    }
   ],
   "source": [
    "Input_dir='../data/raw_images/fine'\n",
    "\n",
    "files = glob.glob(Input_dir + '/*.bmp')\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for file in files:\n",
    "    base = os.path.basename(file)\n",
    "    target = base[0:1]\n",
    "    category = int(target)\n",
    "    test_image = Image.open(file)\n",
    "    test_image = np.array(test_image, dtype=\"float32\")\n",
    "    x_data.append(test_image)\n",
    "    y_data.append(np.array([category]))\n",
    "\n",
    "x_data = np.array(x_data)/255.0\n",
    "y_data = np.array(y_data)\n",
    "y_data = to_categorical(y_data, 10)\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "\n",
    "x_data, y_data = shuffle(x_data, y_data)\n",
    "\n",
    "if (Training_Percentage > 0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=Training_Percentage)\n",
    "else:\n",
    "    X_train = x_data\n",
    "    y_train = y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9278"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6494"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.7216, 0.8471, 0.8157],\n",
       "        [0.7529, 0.8627, 0.8157],\n",
       "        [0.7529, 0.8627, 0.8157],\n",
       "        ...,\n",
       "        [0.7216, 0.8   , 0.7529],\n",
       "        [0.6902, 0.7686, 0.7216],\n",
       "        [0.6588, 0.7373, 0.6902]],\n",
       "\n",
       "       [[0.7529, 0.8627, 0.8157],\n",
       "        [0.7529, 0.8627, 0.8157],\n",
       "        [0.7529, 0.8627, 0.8157],\n",
       "        ...,\n",
       "        [0.7216, 0.8   , 0.7529],\n",
       "        [0.6902, 0.7686, 0.7216],\n",
       "        [0.6588, 0.7373, 0.6588]],\n",
       "\n",
       "       [[0.7529, 0.8627, 0.8157],\n",
       "        [0.7529, 0.8784, 0.8157],\n",
       "        [0.7529, 0.8784, 0.8157],\n",
       "        ...,\n",
       "        [0.7216, 0.8   , 0.7529],\n",
       "        [0.6902, 0.7686, 0.7216],\n",
       "        [0.6588, 0.7216, 0.6588]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.6588, 0.7373, 0.6588],\n",
       "        [0.6275, 0.7216, 0.6588],\n",
       "        [0.5961, 0.7216, 0.6588],\n",
       "        ...,\n",
       "        [0.5961, 0.7059, 0.6275],\n",
       "        [0.5961, 0.6902, 0.6275],\n",
       "        [0.5647, 0.6745, 0.5961]],\n",
       "\n",
       "       [[0.6275, 0.7059, 0.6588],\n",
       "        [0.5961, 0.6902, 0.6275],\n",
       "        [0.5647, 0.6745, 0.6275],\n",
       "        ...,\n",
       "        [0.5961, 0.6745, 0.5961],\n",
       "        [0.5647, 0.6745, 0.5961],\n",
       "        [0.5647, 0.6431, 0.5961]],\n",
       "\n",
       "       [[0.5647, 0.6745, 0.5961],\n",
       "        [0.5333, 0.6588, 0.5961],\n",
       "        [0.502 , 0.6431, 0.5647],\n",
       "        ...,\n",
       "        [0.5333, 0.6431, 0.5647],\n",
       "        [0.5333, 0.6431, 0.5333],\n",
       "        [0.502 , 0.6275, 0.5647]]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "The layout of the network ist a typcial CNN network with alternating **Conv2D** and **MaxPool2D** layers. Finished after **flattening** with additional **Dense** layer.\n",
    "\n",
    "#### Important\n",
    "* Shape of the input layer: (15, 25, 3)\n",
    "* Number of output layers: 10\n",
    "* As loss function \"categorical_crossentropy\" is choosen, as it is a categories task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(25,15,3)),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=3, padding='same', activity_regularizer=regularizers.L2(1e-5),  activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, padding='same'),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=3, padding='same',activity_regularizer=regularizers.L2(1e-5), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, padding='same'),\n",
    "        tf.keras.layers.Conv2D(16, kernel_size=3, padding='same',activity_regularizer=regularizers.L2(1e-5), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, padding='same'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 25, 15, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 8, 32)         18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 4, 16)          4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 33,786\n",
      "Trainable params: 33,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george/anaconda3/envs/gputensorflow/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.RMSprop(lr=1e-4), metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "The input pictures are randomly scattered for brightness, pixel shift variations and rotation angle. This is implemented with a ImageDataGenerator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 2.2898 - accuracy: 0.1453 - val_loss: 2.2694 - val_accuracy: 0.1692\n",
      "Epoch 2/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 2.2248 - accuracy: 0.1732 - val_loss: 2.1436 - val_accuracy: 0.2015\n",
      "Epoch 3/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 2.1250 - accuracy: 0.2065 - val_loss: 2.0381 - val_accuracy: 0.2425\n",
      "Epoch 4/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 2.0519 - accuracy: 0.2469 - val_loss: 1.9530 - val_accuracy: 0.3585\n",
      "Epoch 5/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 1.9476 - accuracy: 0.3030 - val_loss: 1.8136 - val_accuracy: 0.3991\n",
      "Epoch 6/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 1.7702 - accuracy: 0.3949 - val_loss: 1.6486 - val_accuracy: 0.5219\n",
      "Epoch 7/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 1.5003 - accuracy: 0.5087 - val_loss: 1.2695 - val_accuracy: 0.6002\n",
      "Epoch 8/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 1.1981 - accuracy: 0.6108 - val_loss: 0.9683 - val_accuracy: 0.7195\n",
      "Epoch 9/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.9748 - accuracy: 0.6725 - val_loss: 0.7966 - val_accuracy: 0.7360\n",
      "Epoch 10/200\n",
      "2320/2320 [==============================] - 10s 4ms/step - loss: 0.8263 - accuracy: 0.7245 - val_loss: 0.6865 - val_accuracy: 0.7446\n",
      "Epoch 11/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 0.6993 - accuracy: 0.7669 - val_loss: 0.6465 - val_accuracy: 0.7823\n",
      "Epoch 12/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.6050 - accuracy: 0.8021 - val_loss: 0.5118 - val_accuracy: 0.8348\n",
      "Epoch 13/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.5209 - accuracy: 0.8282 - val_loss: 0.3907 - val_accuracy: 0.8836\n",
      "Epoch 14/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.4608 - accuracy: 0.8472 - val_loss: 0.3741 - val_accuracy: 0.8833\n",
      "Epoch 15/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.3942 - accuracy: 0.8699 - val_loss: 0.2597 - val_accuracy: 0.9217\n",
      "Epoch 16/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.3559 - accuracy: 0.8808 - val_loss: 0.2914 - val_accuracy: 0.8998\n",
      "Epoch 17/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.3174 - accuracy: 0.8949 - val_loss: 0.1981 - val_accuracy: 0.9382\n",
      "Epoch 18/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 0.2778 - accuracy: 0.9088 - val_loss: 0.1939 - val_accuracy: 0.9389\n",
      "Epoch 19/200\n",
      "2320/2320 [==============================] - 8s 3ms/step - loss: 0.2509 - accuracy: 0.9185 - val_loss: 0.1762 - val_accuracy: 0.9497\n",
      "Epoch 20/200\n",
      "2320/2320 [==============================] - 8s 3ms/step - loss: 0.2351 - accuracy: 0.9247 - val_loss: 0.1486 - val_accuracy: 0.9569\n",
      "Epoch 21/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.2078 - accuracy: 0.9313 - val_loss: 0.1639 - val_accuracy: 0.9425\n",
      "Epoch 22/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1955 - accuracy: 0.9365 - val_loss: 0.1380 - val_accuracy: 0.9537\n",
      "Epoch 23/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1878 - accuracy: 0.9381 - val_loss: 0.1254 - val_accuracy: 0.9598\n",
      "Epoch 24/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1681 - accuracy: 0.9457 - val_loss: 0.1321 - val_accuracy: 0.9544\n",
      "Epoch 25/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1537 - accuracy: 0.9497 - val_loss: 0.1020 - val_accuracy: 0.9684\n",
      "Epoch 26/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1510 - accuracy: 0.9462 - val_loss: 0.1063 - val_accuracy: 0.9659\n",
      "Epoch 27/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1484 - accuracy: 0.9516 - val_loss: 0.0942 - val_accuracy: 0.9662\n",
      "Epoch 28/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1322 - accuracy: 0.9569 - val_loss: 0.0885 - val_accuracy: 0.9691\n",
      "Epoch 29/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1295 - accuracy: 0.9573 - val_loss: 0.0898 - val_accuracy: 0.9709\n",
      "Epoch 30/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1236 - accuracy: 0.9597 - val_loss: 0.0781 - val_accuracy: 0.9745\n",
      "Epoch 31/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1132 - accuracy: 0.9636 - val_loss: 0.1045 - val_accuracy: 0.9616\n",
      "Epoch 32/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1095 - accuracy: 0.9646 - val_loss: 0.0663 - val_accuracy: 0.9806\n",
      "Epoch 33/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 0.1081 - accuracy: 0.9638 - val_loss: 0.0723 - val_accuracy: 0.9802\n",
      "Epoch 34/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1071 - accuracy: 0.9638 - val_loss: 0.0829 - val_accuracy: 0.9716\n",
      "Epoch 35/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.1009 - accuracy: 0.9678 - val_loss: 0.0579 - val_accuracy: 0.9817\n",
      "Epoch 36/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0932 - accuracy: 0.9699 - val_loss: 0.0677 - val_accuracy: 0.9777\n",
      "Epoch 37/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0965 - accuracy: 0.9684 - val_loss: 0.0698 - val_accuracy: 0.9745\n",
      "Epoch 38/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0904 - accuracy: 0.9683 - val_loss: 0.0757 - val_accuracy: 0.9723\n",
      "Epoch 39/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0927 - accuracy: 0.9712 - val_loss: 0.0773 - val_accuracy: 0.9713\n",
      "Epoch 40/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0884 - accuracy: 0.9706 - val_loss: 0.0550 - val_accuracy: 0.9820\n",
      "Epoch 41/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0900 - accuracy: 0.9710 - val_loss: 0.0586 - val_accuracy: 0.9817\n",
      "Epoch 42/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0797 - accuracy: 0.9737 - val_loss: 0.0554 - val_accuracy: 0.9817\n",
      "Epoch 43/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0772 - accuracy: 0.9735 - val_loss: 0.0522 - val_accuracy: 0.9795\n",
      "Epoch 44/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0776 - accuracy: 0.9749 - val_loss: 0.0848 - val_accuracy: 0.9702\n",
      "Epoch 45/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0758 - accuracy: 0.9747 - val_loss: 0.0518 - val_accuracy: 0.9820\n",
      "Epoch 46/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0842 - accuracy: 0.9724 - val_loss: 0.0424 - val_accuracy: 0.9864\n",
      "Epoch 47/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 0.0743 - accuracy: 0.9753 - val_loss: 0.0598 - val_accuracy: 0.9795\n",
      "Epoch 48/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0690 - accuracy: 0.9759 - val_loss: 0.0413 - val_accuracy: 0.9860\n",
      "Epoch 49/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0712 - accuracy: 0.9768 - val_loss: 0.0497 - val_accuracy: 0.9838\n",
      "Epoch 50/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0769 - accuracy: 0.9756 - val_loss: 0.0565 - val_accuracy: 0.9799\n",
      "Epoch 51/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0738 - accuracy: 0.9749 - val_loss: 0.0384 - val_accuracy: 0.9864\n",
      "Epoch 52/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0690 - accuracy: 0.9793 - val_loss: 0.0418 - val_accuracy: 0.9849\n",
      "Epoch 53/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0708 - accuracy: 0.9752 - val_loss: 0.0356 - val_accuracy: 0.9864\n",
      "Epoch 54/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0646 - accuracy: 0.9775 - val_loss: 0.0435 - val_accuracy: 0.9856\n",
      "Epoch 55/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0672 - accuracy: 0.9768 - val_loss: 0.0661 - val_accuracy: 0.9749\n",
      "Epoch 56/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0675 - accuracy: 0.9778 - val_loss: 0.0395 - val_accuracy: 0.9867\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0584 - accuracy: 0.9814 - val_loss: 0.0477 - val_accuracy: 0.9849\n",
      "Epoch 58/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0630 - accuracy: 0.9803 - val_loss: 0.0399 - val_accuracy: 0.9864\n",
      "Epoch 59/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0632 - accuracy: 0.9793 - val_loss: 0.0483 - val_accuracy: 0.9813\n",
      "Epoch 60/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0626 - accuracy: 0.9810 - val_loss: 0.0404 - val_accuracy: 0.9871\n",
      "Epoch 61/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0623 - accuracy: 0.9807 - val_loss: 0.0355 - val_accuracy: 0.9871\n",
      "Epoch 62/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0594 - accuracy: 0.9815 - val_loss: 0.0382 - val_accuracy: 0.9856\n",
      "Epoch 63/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0652 - accuracy: 0.9782 - val_loss: 0.0546 - val_accuracy: 0.9788\n",
      "Epoch 64/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0642 - accuracy: 0.9794 - val_loss: 0.0330 - val_accuracy: 0.9885\n",
      "Epoch 65/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0602 - accuracy: 0.9806 - val_loss: 0.0446 - val_accuracy: 0.9885\n",
      "Epoch 66/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0645 - accuracy: 0.9811 - val_loss: 0.0317 - val_accuracy: 0.9907\n",
      "Epoch 67/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0534 - accuracy: 0.9818 - val_loss: 0.0347 - val_accuracy: 0.9903\n",
      "Epoch 68/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0338 - val_accuracy: 0.9892\n",
      "Epoch 69/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0503 - accuracy: 0.9848 - val_loss: 0.0382 - val_accuracy: 0.9860\n",
      "Epoch 70/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0568 - accuracy: 0.9818 - val_loss: 0.0423 - val_accuracy: 0.9856\n",
      "Epoch 71/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0494 - accuracy: 0.9839 - val_loss: 0.0425 - val_accuracy: 0.9856\n",
      "Epoch 72/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0520 - accuracy: 0.9825 - val_loss: 0.0477 - val_accuracy: 0.9849\n",
      "Epoch 73/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0574 - accuracy: 0.9836 - val_loss: 0.0336 - val_accuracy: 0.9878\n",
      "Epoch 74/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0520 - accuracy: 0.9846 - val_loss: 0.0357 - val_accuracy: 0.9885\n",
      "Epoch 75/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0546 - accuracy: 0.9825 - val_loss: 0.0567 - val_accuracy: 0.9831\n",
      "Epoch 76/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 0.0528 - accuracy: 0.9820 - val_loss: 0.0573 - val_accuracy: 0.9784\n",
      "Epoch 77/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 0.0513 - accuracy: 0.9838 - val_loss: 0.0412 - val_accuracy: 0.9874\n",
      "Epoch 78/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 0.0589 - accuracy: 0.9826 - val_loss: 0.0370 - val_accuracy: 0.9910\n",
      "Epoch 79/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 0.0490 - accuracy: 0.9830 - val_loss: 0.0285 - val_accuracy: 0.9921\n",
      "Epoch 80/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0546 - accuracy: 0.9823 - val_loss: 0.0363 - val_accuracy: 0.9885\n",
      "Epoch 81/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0607 - accuracy: 0.9802 - val_loss: 0.0337 - val_accuracy: 0.9892\n",
      "Epoch 82/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0479 - accuracy: 0.9839 - val_loss: 0.0263 - val_accuracy: 0.9925\n",
      "Epoch 83/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0463 - accuracy: 0.9847 - val_loss: 0.0394 - val_accuracy: 0.9856\n",
      "Epoch 84/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0528 - accuracy: 0.9831 - val_loss: 0.0494 - val_accuracy: 0.9846\n",
      "Epoch 85/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0513 - accuracy: 0.9840 - val_loss: 0.0318 - val_accuracy: 0.9892\n",
      "Epoch 86/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0502 - accuracy: 0.9821 - val_loss: 0.0279 - val_accuracy: 0.9914\n",
      "Epoch 87/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0476 - accuracy: 0.9847 - val_loss: 0.0713 - val_accuracy: 0.9767\n",
      "Epoch 88/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0573 - accuracy: 0.9836 - val_loss: 0.0335 - val_accuracy: 0.9892\n",
      "Epoch 89/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0544 - accuracy: 0.9819 - val_loss: 0.0310 - val_accuracy: 0.9892\n",
      "Epoch 90/200\n",
      "2320/2320 [==============================] - 9s 4ms/step - loss: 0.0485 - accuracy: 0.9851 - val_loss: 0.0331 - val_accuracy: 0.9910\n",
      "Epoch 91/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0489 - accuracy: 0.9856 - val_loss: 0.0326 - val_accuracy: 0.9892\n",
      "Epoch 92/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0549 - accuracy: 0.9832 - val_loss: 0.0312 - val_accuracy: 0.9907\n",
      "Epoch 93/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0471 - accuracy: 0.9864 - val_loss: 0.0432 - val_accuracy: 0.9864\n",
      "Epoch 94/200\n",
      "2320/2320 [==============================] - 8s 4ms/step - loss: 0.0456 - accuracy: 0.9854 - val_loss: 0.0289 - val_accuracy: 0.9910\n"
     ]
    }
   ],
   "source": [
    "Batch_Size = 4\n",
    "Shift_Range = 1\n",
    "Brightness_Range = 0.1\n",
    "Rotation_Angle = 0\n",
    "ZoomRange = 0.0\n",
    "# Epochs = 10\n",
    "callback = []\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "                    monitor=\"val_accuracy\",\n",
    "                    min_delta=0,\n",
    "                    patience=12,\n",
    "                    verbose=0,\n",
    "                    mode=\"auto\",\n",
    "                    baseline=None,\n",
    "                    restore_best_weights=False,\n",
    "                )\n",
    "\n",
    "# datagen = ImageDataGenerator(width_shift_range=[-Shift_Range,Shift_Range], \n",
    "#                              height_shift_range=[-Shift_Range,Shift_Range],\n",
    "#                              brightness_range=[1-Brightness_Range,1+Brightness_Range],\n",
    "#                              zoom_range=[1-ZoomRange, 1+ZoomRange],\n",
    "#                              rotation_range=Rotation_Angle,vertical_flip=True)\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                                 featurewise_std_normalization=False,\n",
    "                                 rotation_range=.3,\n",
    "                                 width_shift_range=0.4,\n",
    "                                 height_shift_range=0.0,)\n",
    "\n",
    "if (Training_Percentage > 0):\n",
    "    train_iterator = datagen.flow(x_data, y_data, batch_size=Batch_Size, shuffle = True,save_format =\"bmp\" )\n",
    "    validation_iterator = datagen.flow(X_test, y_test, batch_size=Batch_Size, shuffle = False, save_format =\"bmp\")\n",
    "    history = model.fit(train_iterator, validation_data = validation_iterator, epochs = Epochs, callbacks=[early_stopping])\n",
    "else:\n",
    "    train_iterator = datagen.flow(x_data, y_data, batch_size=Batch_Size, shuffle = True,)\n",
    "    history = model.fit(train_iterator, epochs = Epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0050 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "final_loss, final_acc = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"final_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted_model.tflite\r\n",
      "digit_model.h5\r\n",
      "digit_model_quant.tflite\r\n",
      "digit_model_quant.tflite_int8_34.tflite\r\n",
      "digit_model_quant.tflite_int8_42.tflite\r\n",
      "digit_model_quant.tflite_int8.tflite\r\n",
      "digit_model_quant.tflite_int8_v2.tflite\r\n",
      "digit_model_quant.tflite_uint8_v2.tflite\r\n",
      "digit_model_quant_v2.tflite\r\n",
      "draw_digit_with_perpective_transform_crop_save_image_colored1.ipynb\r\n",
      "draw_digit_with_perpective_transform_crop_save_image_colored2.ipynb\r\n",
      "draw_digit_with_perpective_transform_crop_save_image_colored3.ipynb\r\n",
      "draw_digit_with_perpective_transform_crop_save_image_colored4.ipynb\r\n",
      "draw_digit_with_perpective_transform_crop_save_image_colored5.ipynb\r\n",
      "final.h5\r\n",
      "final_v2.h5\r\n",
      "image_augumentation.ipynb\r\n",
      "inference.ipynb\r\n",
      "inference_jpg.ipynb\r\n",
      "\u001b[0m\u001b[01;34moutput\u001b[0m/\r\n",
      "training_notebook2.ipynb\r\n",
      "training_notebook3.ipynb\r\n",
      "training_notebook4.ipynb\r\n",
      "train_notebook_final.ipynb\r\n",
      "walkthrough.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learing result\n",
    " \n",
    "* Visualization of the training and validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_ges = np.append(loss_ges, history.history['loss'])\n",
    "plt.semilogy(history.history['loss'])\n",
    "\n",
    "if (Training_Percentage > 0):\n",
    "    val_loss_ges = np.append(val_loss_ges, history.history['val_loss'])\n",
    "    plt.semilogy(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','eval'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_dir='../data/raw_images/sample_jpg'\n",
    "files = glob.glob(Input_dir + '/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw_images/sample_jpg/3 3\n",
      "../data/raw_images/sample_jpg/7 5\n",
      "../data/raw_images/sample_jpg/1 2\n",
      "../data/raw_images/sample_jpg/5 5\n",
      "../data/raw_images/sample_jpg/1 1\n",
      "../data/raw_images/sample_jpg/4 4\n",
      "../data/raw_images/sample_jpg/0 6\n",
      "../data/raw_images/sample_jpg/2 2\n",
      "../data/raw_images/sample_jpg/8 6\n",
      "../data/raw_images/sample_jpg/7 7\n",
      "../data/raw_images/sample_jpg/4 3\n",
      "../data/raw_images/sample_jpg/7 5\n",
      "../data/raw_images/sample_jpg/5 5\n",
      "../data/raw_images/sample_jpg/9 5\n",
      "../data/raw_images/sample_jpg/0 0\n",
      "../data/raw_images/sample_jpg/8 6\n",
      "../data/raw_images/sample_jpg/6 5\n",
      "../data/raw_images/sample_jpg/4 3\n",
      "../data/raw_images/sample_jpg/9 5\n",
      "../data/raw_images/sample_jpg/7 5\n",
      "../data/raw_images/sample_jpg/8 6\n",
      "../data/raw_images/sample_jpg/2 2\n",
      "../data/raw_images/sample_jpg/3 3\n",
      "../data/raw_images/sample_jpg/4 3\n",
      "../data/raw_images/sample_jpg/5 5\n",
      "../data/raw_images/sample_jpg/6 6\n",
      "../data/raw_images/sample_jpg/8 6\n",
      "../data/raw_images/sample_jpg/8 6\n",
      "../data/raw_images/sample_jpg/6 6\n",
      "../data/raw_images/sample_jpg/9 5\n",
      "../data/raw_images/sample_jpg/0 0\n",
      "../data/raw_images/sample_jpg/1 1\n",
      "../data/raw_images/sample_jpg/2 2\n",
      "../data/raw_images/sample_jpg/3 3\n",
      "../data/raw_images/sample_jpg/4 4\n",
      "../data/raw_images/sample_jpg/9 5\n",
      "../data/raw_images/sample_jpg/1 1\n",
      "../data/raw_images/sample_jpg/2 3\n",
      "../data/raw_images/sample_jpg/2 2\n",
      "../data/raw_images/sample_jpg/3 3\n",
      "../data/raw_images/sample_jpg/1 1\n",
      "../data/raw_images/sample_jpg/7 7\n",
      "../data/raw_images/sample_jpg/6 5\n",
      "../data/raw_images/sample_jpg/5 5\n",
      "../data/raw_images/sample_jpg/6 6\n",
      "../data/raw_images/sample_jpg/0 0\n",
      "../data/raw_images/sample_jpg/0 0\n",
      "../data/raw_images/sample_jpg/5 5\n",
      "../data/raw_images/sample_jpg/9 5\n",
      "../data/raw_images/sample_jpg/3 3\n"
     ]
    }
   ],
   "source": [
    "for file in files[:100]: \n",
    "    base = os.path.basename(file)\n",
    "    target = base[0:1]\n",
    "    image_bmp = Image.open(file)\n",
    "    test_image = np.array(image_bmp, dtype=\"float32\")\n",
    "    img = np.reshape(test_image,[1,25,15,3])\n",
    "    classes = np.argmax(model.predict(img), axis=-1)\n",
    "    classes = classes[0]\n",
    "    print(file[:31], classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "* Save the model to the file with the \"h5\" file format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post training weight quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Depending on your requirements (performance, memory and runtime), post training quantization can be done in two ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach #1:  Post training weight quantization (quantizes weights only) In this case only weights are quantized to int8 but activations remain as they were. Inference input and output are floating-point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6smyl2jg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6smyl2jg/assets\n",
      "2022-10-17 00:44:58.165339: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-10-17 00:44:58.165515: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2022-10-17 00:44:58.167595: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.011ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2022-10-17 00:44:58.218170: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:345] Ignored output_format.\n",
      "2022-10-17 00:44:58.218199: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:348] Ignored drop_control_dependency.\n",
      "2022-10-17 00:44:58.238375: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor sequential_1/dense_3/MatMul because it has fewer than 1024 elements (640).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42928"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.experimental_new_converter = True\n",
    "# Post training quantization\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_quant_model = converter.convert()\n",
    "open(\"digit_model_quant_v2.tflite\", \"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for n in range(10):\n",
    "        data = np.expand_dims(x_data[5], axis=0)\n",
    "        yield [data.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_data).batch(1).take(100):\n",
    "    # Model has only one input so each data point has one element.\n",
    "        yield [input_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach #2: Full integer quantization (Quantizes weights and activations) In this case weights and activations are quantized to int8. First we need to follow the approach #1 to quantize weight and then implement following code to do full integer quantization. This uses quantized input and output, making it compatible with more accelerators, such as the Openmv Camera and Coral Edge TPU. Inference input and output are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpwaxptyc9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpwaxptyc9/assets\n",
      "2022-10-17 01:11:15.155498: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-10-17 01:11:15.155651: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2022-10-17 01:11:15.157378: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.006ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2022-10-17 01:11:15.208229: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:345] Ignored output_format.\n",
      "2022-10-17 01:11:15.208258: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:348] Ignored drop_control_dependency.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n",
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42064"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.target_spec.supported_types = [tf.dtypes.int8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_model_quant = converter.convert()\n",
    "open(\"digit_model_quant.tflite_uint8_v2.tflite\", \"wb\").write(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
